{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacaa399",
   "metadata": {},
   "source": [
    "# Download Kaggle W2 forms and Generate Images from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008c4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import os\n",
    "\n",
    "import ghostscript\n",
    "##### must install ghostscript exe on windows https://ghostscript.com/releases/gsdnld.html #####\n",
    "import numpy\n",
    "from keras.utils import np_utils\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "kaggle_w2_folder = 'enter-path-here'\n",
    "kaggle_w2_images_folder = 'enter-path-here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a200487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset():\n",
    "    os.environ['KAGGLE_USERNAME'] = 'username-here'\n",
    "    os.environ['KAGGLE_KEY'] = 'key-here'\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    dataset = 'mcvishnu1/fake-w2-us-tax-form-dataset'\n",
    "    path = 'datasets/fake_w2'\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    api.dataset_download_file(dataset, 'W2_Multi_Clean_DataSet_02.ZIP', path)\n",
    "\n",
    "def extract_kaggle_dataset():\n",
    "    from zipfile import ZipFile\n",
    "    file_name = \"C:/datasets/fake_w2/W2_Multi_Clean_DataSet_02.ZIP\"\n",
    "    os.chdir('enter-path-here')\n",
    "\n",
    "    with ZipFile(file_name, 'r') as zip:\t\n",
    "        zip.printdir()\t\n",
    "        print('Extracting all the files now...')\n",
    "        zip.extractall()\n",
    "        print('Done!')\n",
    "\n",
    "def convert_pdfs_to_images():\n",
    "    def pdf2jpeg(pdf_input_path, jpeg_output_path):\n",
    "        args = [\"pef2jpeg\", # actual value doesn't matter\n",
    "                \"-dNOPAUSE\",\n",
    "                \"-sDEVICE=jpeg\",\n",
    "                \"-r144\",\n",
    "                \"-sOutputFile=\" + jpeg_output_path,\n",
    "                pdf_input_path]\n",
    "\n",
    "        encoding = locale.getpreferredencoding()\n",
    "        args = [a.encode(encoding) for a in args]\n",
    "\n",
    "        ghostscript.Ghostscript(*args)\n",
    "\n",
    "    def get_pdf_files():\n",
    "        return os.listdir(kaggle_w2_folder) \n",
    "\n",
    "    [pdf2jpeg(\n",
    "        kaggle_w2_folder+pdf_path,\n",
    "        kaggle_w2_images_folder+pdf_path.replace('pdf','jpeg')) \n",
    "            for pdf_path in get_pdf_files() if pdf_path.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253924cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading kaggle dataset as zip\n",
      "extracting data from zipped file\n",
      "converting pdfs to images\n",
      "skip this cell after running it once\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('downloading kaggle dataset as zip')\n",
    "download_kaggle_dataset()\n",
    "\n",
    "print('extracting data from zipped file')\n",
    "extract_kaggle_dataset()\n",
    "\n",
    "print('converting pdfs to images')\n",
    "convert_pdfs_to_images()\n",
    "\n",
    "print('skip this cell after running it once')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c23d83",
   "metadata": {},
   "source": [
    "# Images EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850f8c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "found 1000 images.\n",
      "Using 8 threads. (max:8)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:48<00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*--------------------------------------------------------------------------------------*\n",
      "number of images                         |  1000\n",
      "\n",
      "dtype                                    |  uint8\n",
      "channels                                 |  [3]\n",
      "extensions                               |  ['jpeg']\n",
      "\n",
      "min height                               |  1584\n",
      "max height                               |  1584\n",
      "mean height                              |  1584.0\n",
      "median height                            |  1584\n",
      "\n",
      "min width                                |  1224\n",
      "max width                                |  1224\n",
      "mean width                               |  1224.0\n",
      "median width                             |  1224\n",
      "\n",
      "mean height/width ratio                  |  1.2941176470588236\n",
      "median height/width ratio                |  1.2941176470588236\n",
      "recommended input size(by mean)          |  [1584 1224] (h x w, multiples of 8)\n",
      "recommended input size(by mean)          |  [1584 1216] (h x w, multiples of 16)\n",
      "recommended input size(by mean)          |  [1600 1216] (h x w, multiples of 32)\n",
      "\n",
      "channel mean(0~1)                        |  [0.9399718  0.9399661  0.93996894]\n",
      "channel std(0~1)                         |  [0.23087765 0.23089147 0.23088476]\n",
      "*--------------------------------------------------------------------------------------*\n",
      "eda ended in 00 hours 00 minutes 48 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dtype': 'uint8',\n",
       " 'channels': [3],\n",
       " 'extensions': ['jpeg'],\n",
       " 'min_h': 1584,\n",
       " 'max_h': 1584,\n",
       " 'mean_h': 1584.0,\n",
       " 'median_h': 1584,\n",
       " 'min_w': 1224,\n",
       " 'max_w': 1224,\n",
       " 'mean_w': 1224.0,\n",
       " 'median_w': 1224,\n",
       " 'mean_hw_ratio': 1.2941176470588236,\n",
       " 'median_hw_ratio': 1.2941176470588236,\n",
       " 'rec_hw_size_8': array([1584, 1224]),\n",
       " 'rec_hw_size_16': array([1584, 1216]),\n",
       " 'rec_hw_size_32': array([1600, 1216]),\n",
       " 'mean': array([0.9399718 , 0.9399661 , 0.93996894], dtype=float32),\n",
       " 'std': array([0.23087765, 0.23089147, 0.23088476], dtype=float32)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from basic_image_eda import BasicImageEDA\n",
    "\n",
    "BasicImageEDA.explore(kaggle_w2_images_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85ffe6",
   "metadata": {},
   "source": [
    "# Build TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fe2fc",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6cf8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_arrays():\n",
    "    print('sorting w2 class files into arrays')\n",
    "    adp1s,adp2s,irs1s,irs2s = [],[],[],[]\n",
    "    for path in os.listdir(kaggle_w2_images_folder):    \n",
    "        if 'adp1' in path.lower():\n",
    "            adp1s.append(kaggle_w2_images_folder+path)\n",
    "        if 'adp2' in path.lower():\n",
    "            adp2s.append(kaggle_w2_images_folder+path)\n",
    "        if 'irs1' in path.lower():\n",
    "            irs1s.append(kaggle_w2_images_folder+path)\n",
    "        if 'irs2' in path.lower():\n",
    "            irs2s.append(kaggle_w2_images_folder+path)\n",
    "    assert len(adp1s) == 250\n",
    "    assert len(adp2s) == 250\n",
    "    assert len(irs1s) == 250\n",
    "    assert len(irs2s) == 250    \n",
    "    return adp1s,adp2s,irs1s,irs2s\n",
    "\n",
    "def get_training_features_labels(training_files):\n",
    "    features = []\n",
    "\n",
    "    for training_file in training_files:\n",
    "        features.append(numpy.array(Image.open(training_file)))\n",
    "\n",
    "    X_train = numpy.array(features).reshape(len(features),1584,1224,3)\n",
    "    y_train = np_utils.to_categorical(\n",
    "        numpy.array([0 for i in range(0,25)] + [1 for i in range(0,25)] + [2 for i in range(0,25)] + [3 for i in range(0,25)]))\n",
    "\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_train) == 100\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "def get_test_features_labels(test_files):\n",
    "    features = []\n",
    "\n",
    "    for test_file in test_files:\n",
    "        features.append(numpy.array(Image.open(test_file))) \n",
    "\n",
    "    X_train = numpy.array(features).reshape(len(features),1584,1224,3)\n",
    "    y_train = np_utils.to_categorical(\n",
    "        numpy.array([0 for i in range(0,25)] + [1 for i in range(0,25)] + [2 for i in range(0,25)] + [3 for i in range(0,25)]))\n",
    "\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_train) == 100\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "def get_label(image_path):\n",
    "    image_path = image_path.lower()\n",
    "    \n",
    "    if 'adp1' in image_path:\n",
    "        return 0\n",
    "    if 'adp2' in image_path:\n",
    "        return 1\n",
    "    if 'irs1' in image_path:\n",
    "        return 2\n",
    "    if 'irs2' in image_path:\n",
    "        return 3\n",
    "\n",
    "    raise Exception(f'label not found in {image_path}')\n",
    "\n",
    "def get_predictions_and_truth(model,_training_files):\n",
    "    print('making predictions')\n",
    "    y_predictions = []\n",
    "    y_true = []\n",
    "\n",
    "    for path in os.listdir(kaggle_w2_images_folder):\n",
    "        if kaggle_w2_images_folder + path in _training_files:\n",
    "            print(f'skipping image used for training: {path}')\n",
    "            continue\n",
    "        w2_features = numpy.array(\n",
    "            Image.open(kaggle_w2_images_folder + path))\\\n",
    "                .reshape(1,1584,1224,3)\n",
    "        print(f'running prediction on image: {path}')\n",
    "        y_predictions.append(model.predict(w2_features)[0])\n",
    "        y_true.append(get_label(path))\n",
    "\n",
    "    assert len(y_predictions) == len(y_true)\n",
    "    y_true_categorical = np_utils.to_categorical(y_true)\n",
    "    return y_true_categorical,numpy.array(y_predictions)\n",
    "\n",
    "def get_model(num_classes):\n",
    "    print(f'number of classes in tf.kears: {num_classes}')\n",
    "    print('preping model')\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(1584,1224,3)),\n",
    "        tf.keras.layers.Conv2D(9,5,5, padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2,2), padding='same'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')])\n",
    "\n",
    "    print('compiling model')\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer='adam', \n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_training_test_file_paths(): \n",
    "    adp1s,adp2s,irs1s,irs2s = get_class_arrays()\n",
    "    training_files = adp1s[:25] + adp2s[:25] + irs1s[:25] + irs2s[:25]\n",
    "    test_files = adp1s[26:51] + adp2s[26:51] + irs1s[26:51] + irs2s[26:51]\n",
    "    return training_files,test_files\n",
    "\n",
    "def print_model_results(y_true_categorical,y_predictions):\n",
    "    y_true_categorical = y_true_categorical.astype('int32')\n",
    "    y_predictions = y_predictions.astype('int32')\n",
    "    \n",
    "    print('getting model stats')\n",
    "    metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    metric.update_state(y_true_categorical,y_predictions)\n",
    "    print('categorical accuracy')\n",
    "    print(metric.result().numpy())\n",
    "\n",
    "    accuracy_score_ = accuracy_score(y_true_categorical, y_predictions)\n",
    "    print(f\"Test Accuracy : {accuracy_score_}\")\n",
    "\n",
    "    print(\"Classification Report :\")\n",
    "    print(classification_report(\n",
    "        y_true_categorical, \n",
    "        y_predictions, \n",
    "        target_names=['adp1','adp2','irs1','irs2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f63ee",
   "metadata": {},
   "source": [
    "# Model Training/Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "271a5cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting w2 class files into arrays\n",
      "number of classes in tf.kears: 4\n",
      "preping model\n",
      "compiling model\n",
      "training model\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 6746.8677 - accuracy: 0.6900 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 9s 175ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "making predictions\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15500.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15501.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15502.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15503.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15504.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15505.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15506.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15507.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15508.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15509.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15510.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15511.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15512.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15513.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15514.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15515.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15516.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15517.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15518.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15519.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15520.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15521.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15522.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15523.jpeg\n",
      "skipping image used for training: W2_Multi_Sample_Data_input_ADP1_clean_15524.jpeg\n",
      "running prediction on image: W2_Multi_Sample_Data_input_ADP1_clean_15525.jpeg\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "running prediction on image: W2_Multi_Sample_Data_input_ADP1_clean_15526.jpeg\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "running prediction on image: W2_Multi_Sample_Data_input_ADP1_clean_15527.jpeg\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "running prediction on image: W2_Multi_Sample_Data_input_ADP1_clean_15528.jpeg\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "running prediction on image: W2_Multi_Sample_Data_input_ADP1_clean_15529.jpeg\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "running prediction on image: W2_Multi_Sample_Data_input_ADP1_clean_15530.jpeg\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "running prediction on image: W2_Multi_Sample_Data_input_ADP1_clean_15531.jpeg\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "running prediction on image: W2_Multi_Sample_Data_input_IRS2_clean_10499.jpeg\n",
      "1/1 [==============================] - 0s 61ms/step\n"
     ]
    }
   ],
   "source": [
    "training_files,test_files = get_training_test_file_paths()\n",
    "X_train, y_train = get_training_features_labels(training_files)\n",
    "X_test, y_test = get_test_features_labels(test_files)\n",
    "\n",
    "num_classes=y_train.shape[1]\n",
    "\n",
    "model = get_model(num_classes)\n",
    "\n",
    "print('training model')\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=2,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test))\n",
    "\n",
    "y_true_categorical,y_predictions= get_predictions_and_truth(model,training_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72d2c6",
   "metadata": {},
   "source": [
    "# Model Results\n",
    "\n",
    "Perfect F1-score :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5246b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting model stats\n",
      "categorical accuracy\n",
      "1.0\n",
      "Test Accuracy : 1.0\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        adp1       1.00      1.00      1.00       225\n",
      "        adp2       1.00      1.00      1.00       225\n",
      "        irs1       1.00      1.00      1.00       225\n",
      "        irs2       1.00      1.00      1.00       225\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       900\n",
      "   macro avg       1.00      1.00      1.00       900\n",
      "weighted avg       1.00      1.00      1.00       900\n",
      " samples avg       1.00      1.00      1.00       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_model_results(\n",
    "    y_true_categorical,\n",
    "    y_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('sureprep': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e4a2899b772ca907ac00d33a278a658390cb184ce84c47eaf8197db8f2e9992"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
